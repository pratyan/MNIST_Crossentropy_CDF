# -*- coding: utf-8 -*-
"""MNIST_CDF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QMejxS-3APMc4QRsc22CreMR5RZdaiAV
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import math



class classifier:
  def __init__(self, x_train, y_train, x_test, y_test, rate, epoch):

    self.x_train = tf.Variable(x_train, dtype=tf.float32)
    self.y_train = tf.Variable(y_train, dtype=tf.int64)

    self.x_test = tf.Variable(x_test, dtype=tf.float32)
    self.y_test = tf.Variable(y_test, dtype=tf.int64)

    self.x_train = tf.reshape(self.x_train, shape=[-1,784])/255
    self.x_test =tf.reshape(self.x_test, shape=[-1, 784])/255

    self.rate = rate 
    self.epoch = epoch

    self.init_weight = tf.ones(shape=[784,10], dtype=tf.float32)
    self.init_bias = tf.ones(shape=[10], dtype=tf.float32)

    self.weight = tf.Variable(self.init_weight, dtype=tf.float32)
    self.bias = tf.Variable(self.init_bias, dtype=tf.float32)

    self.optimizer=tf.keras.optimizers.Adam(learning_rate=self.rate)
    self.loss=tf.keras.losses.CategoricalCrossentropy()

    self.actual_test_label = self.y_test


  def calculate(self,data):
    calc_value=tf.linalg.matmul(data,self.weight)+self.bias
    return calc_value


  def one_hot(self):
    self.y_train = tf.one_hot(self.y_train, depth=10)
    self.y_test = tf.one_hot(self.y_test, depth=10)


  def CDF(self,val):
    std=tf.math.reduce_std(val)
    mean=tf.math.reduce_mean(val)
    return 0.5*(1+tf.math.erf((val-mean)/(std*math.sqrt(2))))

  def predict(self, data):
    return self.CDF(self.calculate(data))

  @tf.function
  def train(self):
    loss_fn = lambda : self.loss(self.y_train, self.predict(self.x_train))
    var = [self.weight, self.bias]

    for i in range(self.epoch):
      self.optimizer.minimize(loss = loss_fn, var_list = var)
      tf.print("loss: ", self.loss(self.y_train, self.predict(self.x_train)))


  def accuracy(self):
        output=[tf.math.argmax(val).numpy() for val in self.predict(self.x_test)]
        true_labels=[val for val in self.actual_test_label.numpy()]
        count=0
        for i in range(len(output)):
            if(output[i]!=true_labels[i]):
                count+=1
        print((10000-count)/10000)

Classifier =classifier(x_train, y_train, x_test, y_test, 0.3, 500)

Classifier.one_hot()
Classifier.train()

Classifier.accuracy()


